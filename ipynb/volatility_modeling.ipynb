{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Modeling - Part 1\n",
    "## Recap, Stylized Facts, EWMA and GARCH Models\n",
    "\n",
    "This notebook covers:\n",
    "1. **Volatility Modeling Recap**: EWMA, GARCH and related models\n",
    "2. **Stylized Facts**: Key properties of financial volatility\n",
    "3. **EWMA Model**: Exponentially Weighted Moving Average\n",
    "4. **GARCH Models**: Manual optimization and arch package implementation\n",
    "5. **Realized Volatility**: Comparison with model-based estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install arch yfinance wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from arch import arch_model\n",
    "import zipfile\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "### S&P 500 Returns from WRDS\n",
    "\n",
    "We'll use WRDS (Wharton Research Data Services) to access high-quality financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to WRDS and download S&P 500 data\n",
    "import wrds\n",
    "\n",
    "# Connect to WRDS (you might have to confirm with 2FA)\n",
    "db = wrds.Connection(wrds_username=None)\n",
    "\n",
    "# Download S&P 500 index data from Compustat\n",
    "# Date range: 2000-01-03 to 2022-06-28 (matching RV5 data)\n",
    "query = \"\"\"\n",
    "SELECT datadate as date, prccd as price\n",
    "FROM comp.idx_daily\n",
    "WHERE gvkeyx = '000003'\n",
    "  AND datadate >= '2000-01-03' \n",
    "  AND datadate <= '2022-06-28'\n",
    "ORDER BY datadate\n",
    "\"\"\"\n",
    "\n",
    "print(\"Downloading S&P 500 data from WRDS/Compustat...\")\n",
    "data = db.raw_sql(query)\n",
    "db.close()\n",
    "\n",
    "# Process data\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data = data.set_index('date')\n",
    "\n",
    "# Calculate log returns (in percentage)\n",
    "returns = 100 * np.log(data['price'] / data['price'].shift(1))\n",
    "returns = returns.dropna()\n",
    "\n",
    "print(f\"\\nData loaded: {len(returns)} observations\")\n",
    "print(f\"Date range: {returns.index[0]} to {returns.index[-1]}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(returns.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative: Using yfinance (uncomment to use)\n",
    "# import yfinance as yf\n",
    "\n",
    "# # Download S&P 500 data\n",
    "# print(\"Downloading S&P 500 data from Yahoo Finance...\")\n",
    "# sp500 = yf.download('^GSPC', start='2000-01-01', end='2024-12-31', progress=False)\n",
    "\n",
    "# # Calculate log returns (in percentage)\n",
    "# returns = 100 * np.log(sp500['Close'] / sp500['Close'].shift(1))\n",
    "# returns = returns.dropna()\n",
    "# returns.name = 'returns'\n",
    "# returns = returns['^GSPC']\n",
    "\n",
    "# print(f\"\\nData loaded: {len(returns)} observations\")\n",
    "# print(f\"Date range: {returns.index[0]} to {returns.index[-1]}\")\n",
    "# print(f\"\\nBasic statistics:\")\n",
    "# print(returns.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stylized Facts of Volatility\n",
    "\n",
    "Financial volatility exhibits several well-documented empirical regularities:\n",
    "\n",
    "1. **Volatility Clustering**: High (low) volatility tends to be followed by high (low) volatility\n",
    "2. **Fat Tails**: Return distributions have heavier tails than the normal distribution\n",
    "3. **Leverage Effect**: Negative returns are associated with larger increases in volatility than positive returns\n",
    "4. **Mean Reversion**: Volatility tends to revert to a long-run mean\n",
    "5. **Long Memory**: Volatility shocks persist for extended periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize returns and distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Time series plot\n",
    "axes[0, 0].plot(returns.index, returns.values, linewidth=0.5, color='navy', alpha=0.7)\n",
    "axes[0, 0].set_title('S&P 500 Daily Returns', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Return (%)')\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=0.5)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram with normal overlay\n",
    "axes[0, 1].hist(returns.values, bins=100, alpha=0.7, color='navy', density=True, edgecolor='black')\n",
    "mu, std = returns.mean(), returns.std()\n",
    "x = np.linspace(returns.min(), returns.max(), 100)\n",
    "axes[0, 1].plot(x, stats.norm.pdf(x, mu, std), 'r-', linewidth=2, label='Normal Distribution')\n",
    "axes[0, 1].set_title('Distribution of Returns', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Return (%)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Absolute returns (proxy for volatility)\n",
    "abs_returns = np.abs(returns)\n",
    "axes[1, 0].plot(abs_returns.index, abs_returns.values, linewidth=0.5, color='darkred', alpha=0.7)\n",
    "axes[1, 0].set_title('Absolute Returns (Volatility Proxy)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('|Return| (%)')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ACF of absolute returns (volatility clustering)\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(abs_returns.values, lags=50, ax=axes[1, 1], alpha=0.05)\n",
    "axes[1, 1].set_title('ACF of Absolute Returns', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Lag')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STYLIZED FACTS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean return: {returns.mean():.4f}%\")\n",
    "print(f\"Volatility (std): {returns.std():.4f}%\")\n",
    "print(f\"Skewness: {returns.skew():.4f} (normal = 0)\")\n",
    "print(f\"Excess Kurtosis: {returns.kurtosis():.4f} (normal = 0)\")\n",
    "print(f\"\\n✓ Fat tails confirmed: Excess kurtosis = {returns.kurtosis():.2f} >> 0\")\n",
    "print(f\"✓ Volatility clustering visible in absolute returns plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EWMA (Exponentially Weighted Moving Average)\n",
    "\n",
    "### Simple Volatility Model\n",
    "\n",
    "The EWMA model is a simple approach for estimating volatility that gives more weight to recent observations:\n",
    "\n",
    "$$\\sigma_t^2 = \\lambda \\sigma_{t-1}^2 + (1-\\lambda) r_{t-1}^2$$\n",
    "\n",
    "where:\n",
    "- $\\lambda$: Decay factor (typically 0.94 for daily data, RiskMetrics uses 0.94)\n",
    "- $\\sigma_t^2$: Conditional variance at time t\n",
    "- $r_t$: Return at time t\n",
    "\n",
    "**Key Properties:**\n",
    "- Simple recursive formula\n",
    "- No parameters to estimate (λ often fixed)\n",
    "- More weight on recent observations\n",
    "- Special case of IGARCH (Integrated GARCH) with $\\alpha + \\beta = 1$\n",
    "\n",
    "**RiskMetrics (J.P. Morgan):**\n",
    "- Popularized EWMA for VaR calculations\n",
    "- Recommended $\\lambda = 0.94$ for daily data\n",
    "- Recommended $\\lambda = 0.97$ for monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.univariate import EWMAVariance, ConstantMean\n",
    "\n",
    "mdl = ConstantMean(returns, volatility=EWMAVariance(0.94))\n",
    "ewma_fit = mdl.fit()\n",
    "ewma_fit.summary()\n",
    "ewma_vol = ewma_fit.conditional_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fitted volatilities using EWMA parameters\n",
    "T = len(returns)\n",
    "sigma2_manual = np.zeros(T)\n",
    "sigma2_manual[0] = returns.var()\n",
    "for t in range(1, T):\n",
    "    sigma2_manual[t] = 0.06 * (returns.iloc[t-1])**2 + 0.94 * sigma2_manual[t-1]\n",
    "\n",
    "ewma_vol_manual = pd.Series(np.sqrt(sigma2_manual), index=returns.index, name='ewma_manual')\n",
    "ewma_vol_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GARCH Models\n",
    "\n",
    "### GARCH(1,1) Model\n",
    "\n",
    "The GARCH(1,1) model specifies conditional variance as:\n",
    "\n",
    "$$\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2$$\n",
    "\n",
    "where:\n",
    "- $\\omega > 0$: Long-run variance component\n",
    "- $\\alpha \\geq 0$: Reaction to market shocks\n",
    "- $\\beta \\geq 0$: Persistence of volatility\n",
    "- $\\alpha + \\beta < 1$: Stationarity condition\n",
    "\n",
    "**Key Properties:**\n",
    "- Captures volatility clustering\n",
    "- Mean-reverting to long-run variance: $\\sigma_L^2 = \\omega / (1 - \\alpha - \\beta)$\n",
    "- Persistence: $\\alpha + \\beta$ (close to 1 indicates high persistence)\n",
    "\n",
    "**Estimation:**\n",
    "Maximum Likelihood Estimation (MLE) assuming normal innovations:\n",
    "\n",
    "$$\\log L(\\theta) = -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\log(2\\pi) + \\log(\\sigma_t^2) + \\frac{r_t^2}{\\sigma_t^2}\\right]$$\n",
    "\n",
    "where $\\theta = (\\omega, \\alpha, \\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual GARCH(1,1) Estimation\n",
    "\n",
    "First, we demonstrate how to estimate GARCH parameters by manually optimizing the log-likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual GARCH(1,1) estimation via log-likelihood optimization\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def garch_log_likelihood(params, returns):\n",
    "    \"\"\"\n",
    "    Compute negative log-likelihood for GARCH(1,1) model\n",
    "    \"\"\"\n",
    "    mu, omega, alpha, beta = params\n",
    "    T = len(returns)\n",
    "    \n",
    "    # Initialize variance\n",
    "    sigma2 = np.zeros(T)\n",
    "    sigma2[0] = returns.var()  # Unconditional variance\n",
    "    \n",
    "    # Compute conditional variances\n",
    "    for t in range(1, T):\n",
    "        sigma2[t] = omega + alpha * (returns.iloc[t-1] - mu)**2 + beta * sigma2[t-1]\n",
    "    \n",
    "    # Compute log-likelihood\n",
    "    log_lik = -0.5 * np.sum(np.log(2 * np.pi) + np.log(sigma2) + (returns-mu)**2 / sigma2)\n",
    "    \n",
    "    # Return negative log-likelihood (for minimization)\n",
    "    return -log_lik\n",
    "\n",
    "# Initial parameter values\n",
    "initial_params = [0.01, 0.01, 0.05, 0.90]\n",
    "\n",
    "# Constraints: omega > 0, alpha >= 0, beta >= 0, alpha + beta < 1\n",
    "constraints = (\n",
    "    {'type': 'ineq', 'fun': lambda x: x[1]},  # omega > 0\n",
    "    {'type': 'ineq', 'fun': lambda x: x[2]},  # alpha >= 0\n",
    "    {'type': 'ineq', 'fun': lambda x: x[3]},  # beta >= 0\n",
    "    {'type': 'ineq', 'fun': lambda x: 1 - x[2] - x[3]}  # alpha + beta < 1\n",
    ")\n",
    "\n",
    "# Bounds for parameters\n",
    "bounds = ((None,None), (1e-6, None), (0, 1), (0, 1))\n",
    "\n",
    "print(\"Optimizing GARCH(1,1) parameters manually...\")\n",
    "print(\"Initial parameters: mu= {:.4f}, ω={:.4f}, α={:.4f}, β={:.4f}\\n\".format(*initial_params))\n",
    "\n",
    "# Optimize\n",
    "result = minimize(garch_log_likelihood, initial_params, args=(returns,),\n",
    "                  method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "# Extract optimal parameters\n",
    "mu_manual, omega_manual, alpha_manual, beta_manual = result.x\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MANUAL GARCH(1,1) ESTIMATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Optimization successful: {result.success}\")\n",
    "print(f\"Log-likelihood: {-result.fun:.2f}\")\n",
    "print(f\"\\nOptimal parameters:\")\n",
    "print(f\"mu:    {mu_manual:.6f}\")\n",
    "print(f\"ω (omega):    {omega_manual:.6f}\")\n",
    "print(f\"α (alpha):    {alpha_manual:.6f}\")\n",
    "print(f\"β (beta):     {beta_manual:.6f}\")\n",
    "print(f\"\\nPersistence (α+β): {alpha_manual + beta_manual:.6f}\")\n",
    "print(f\"Long-run volatility: {np.sqrt(omega_manual / (1 - alpha_manual - beta_manual)):.4f}%\")\n",
    "\n",
    "# Compute fitted volatilities using manual parameters\n",
    "T = len(returns)\n",
    "sigma2_manual = np.zeros(T)\n",
    "sigma2_manual[0] = returns.var()\n",
    "for t in range(1, T):\n",
    "    sigma2_manual[t] = omega_manual + alpha_manual * (returns.iloc[t-1]-mu_manual)**2 + beta_manual * sigma2_manual[t-1]\n",
    "\n",
    "garch_vol_manual = pd.Series(np.sqrt(sigma2_manual), index=returns.index, name='garch_manual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `arch` Package\n",
    "\n",
    "Now we use the professional `arch` package for comparison and to get additional statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GARCH Models Recap\n",
    "\n",
    "### GARCH(1,1) Model\n",
    "\n",
    "The GARCH(1,1) model specifies conditional variance as:\n",
    "\n",
    "$$\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2$$\n",
    "\n",
    "where:\n",
    "- $\\omega > 0$: Long-run variance component\n",
    "- $\\alpha \\geq 0$: Reaction to market shocks\n",
    "- $\\beta \\geq 0$: Persistence of volatility\n",
    "- $\\alpha + \\beta < 1$: Stationarity condition\n",
    "\n",
    "**Key Properties:**\n",
    "- Captures volatility clustering\n",
    "- Mean-reverting to long-run variance: $\\sigma_L^2 = \\omega / (1 - \\alpha - \\beta)$\n",
    "- Persistence: $\\alpha + \\beta$ (close to 1 indicates high persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate GARCH(1,1) model\n",
    "print(\"Estimating GARCH(1,1) model...\\n\")\n",
    "model_garch = arch_model(returns, vol='Garch', p=1, q=1, dist='normal')\n",
    "garch_fit = model_garch.fit(disp='off')\n",
    "\n",
    "print(garch_fit.summary())\n",
    "\n",
    "# Extract parameters\n",
    "omega = garch_fit.params['omega']\n",
    "alpha = garch_fit.params['alpha[1]']\n",
    "beta = garch_fit.params['beta[1]']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GARCH(1,1) PARAMETERS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ω (omega):           {omega:.6f}\")\n",
    "print(f\"α (alpha):           {alpha:.6f}\")\n",
    "print(f\"β (beta):            {beta:.6f}\")\n",
    "print(f\"\\nPersistence (α+β):   {alpha + beta:.6f}\")\n",
    "print(f\"Long-run variance:   {omega / (1 - alpha - beta):.6f}\")\n",
    "print(f\"Long-run volatility: {np.sqrt(omega / (1 - alpha - beta)):.4f}%\")\n",
    "\n",
    "# Extract conditional volatility\n",
    "garch_vol = garch_fit.conditional_volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GJR-GARCH Model - Leverage Effect\n",
    "\n",
    "The GJR-GARCH model (Glosten, Jagannathan, Runkle, 1993) captures asymmetric volatility:\n",
    "\n",
    "$$\\sigma_t^2 = \\omega + \\alpha\\varepsilon_{t-1}^2 + \\gamma\\varepsilon_{t-1}^2\\mathbb{I}_{\\varepsilon_{t-1}<0} + \\beta \\sigma_{t-1}^2$$\n",
    "\n",
    "where $\\mathbb{I}_{\\varepsilon_{t-1}<0}$ is an indicator function for negative shocks.\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\gamma > 0$: Negative shocks increase volatility more than positive shocks\n",
    "- Impact of positive shock: $\\alpha$\n",
    "- Impact of negative shock: $\\alpha + \\gamma$\n",
    "- This captures the \"leverage effect\" in equity markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate GJR-GARCH(1,1) model\n",
    "print(\"Estimating GJR-GARCH(1,1) model...\\n\")\n",
    "model_gjr = arch_model(returns, vol='GARCH', p=1, o=1, q=1, dist='normal')\n",
    "gjr_fit = model_gjr.fit(disp='off')\n",
    "\n",
    "print(gjr_fit.summary())\n",
    "\n",
    "# Extract parameters\n",
    "omega_gjr = gjr_fit.params['omega']\n",
    "alpha_gjr = gjr_fit.params['alpha[1]']\n",
    "gamma_gjr = gjr_fit.params['gamma[1]']\n",
    "beta_gjr = gjr_fit.params['beta[1]']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GJR-GARCH PARAMETERS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ω (omega):                    {omega_gjr:.6f}\")\n",
    "print(f\"α (alpha):                    {alpha_gjr:.6f}\")\n",
    "print(f\"γ (gamma):                    {gamma_gjr:.6f}\")\n",
    "print(f\"β (beta):                     {beta_gjr:.6f}\")\n",
    "print(f\"\\nTotal impact (negative):      {alpha_gjr + gamma_gjr:.6f}\")\n",
    "print(f\"Asymmetry ratio:              {(alpha_gjr + gamma_gjr) / alpha_gjr:.3f}\")\n",
    "\n",
    "if gamma_gjr > 0:\n",
    "    print(f\"\\n✓ Leverage effect confirmed: γ = {gamma_gjr:.6f} > 0\")\n",
    "    print(f\"  Negative shocks increase volatility {((alpha_gjr + gamma_gjr) / alpha_gjr - 1) * 100:.1f}% more\")\n",
    "\n",
    "# Extract conditional volatility\n",
    "gjr_vol = gjr_fit.conditional_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GARCH and GJR-GARCH volatility estimates\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# Plot both volatility series\n",
    "ax.plot(garch_vol.index, garch_vol.values, linewidth=1, label='GARCH(1,1)', alpha=0.8)\n",
    "ax.plot(gjr_vol.index, gjr_vol.values, linewidth=1, label='GJR-GARCH(1,1)', alpha=0.8)\n",
    "ax.plot(ewma_vol_manual.index, ewma_vol_manual.values, linewidth=1, label='EWMA', alpha=0.8)\n",
    "ax.set_title('Conditional Volatility: EWMA vs GARCH vs GJR-GARCH', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Volatility (%)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and heavy tails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate GARCH(1,1) model\n",
    "print(\"Estimating GARCH(1,1)-t model...\\n\")\n",
    "model_garch_t = arch_model(returns, vol='Garch', p=1, q=1, dist='StudentsT')\n",
    "garch_t_fit = model_garch_t.fit(disp='off')\n",
    "\n",
    "print(garch_t_fit.summary())\n",
    "\n",
    "# Extract parameters\n",
    "omega_t = garch_t_fit.params['omega']\n",
    "alpha_t = garch_t_fit.params['alpha[1]']\n",
    "beta_t = garch_t_fit.params['beta[1]']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GARCH(1,1) PARAMETERS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ω (omega):           {omega_t:.6f}\")\n",
    "print(f\"α (alpha):           {alpha_t:.6f}\")\n",
    "print(f\"β (beta):            {beta_t:.6f}\")\n",
    "print(f\"\\nPersistence (α+β):   {alpha_t + beta_t:.6f}\")\n",
    "print(f\"Long-run variance:   {omega_t / (1 - alpha_t - beta_t):.6f}\")\n",
    "print(f\"Long-run volatility: {np.sqrt(omega_t / (1 - alpha_t - beta_t)):.4f}%\")\n",
    "\n",
    "# Extract conditional volatility\n",
    "garch_vol_t = garch_t_fit.conditional_volatility\n",
    "\n",
    "# Estimate GJR-GARCH(1,1)-t model\n",
    "print(\"Estimating GJR-GARCH(1,1) model...\\n\")\n",
    "model_gjr_t = arch_model(returns, vol='GARCH', p=1, o=1, q=1, dist='StudentsT')\n",
    "gjr_t_fit = model_gjr_t.fit(disp='off')\n",
    "\n",
    "print(gjr_fit.summary())\n",
    "\n",
    "# Extract parameters\n",
    "omega_gjr_t = gjr_t_fit.params['omega']\n",
    "alpha_gjr_t = gjr_t_fit.params['alpha[1]']\n",
    "gamma_gjr_t = gjr_t_fit.params['gamma[1]']\n",
    "beta_gjr_t = gjr_t_fit.params['beta[1]']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GJR-GARCH PARAMETERS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ω (omega):                    {omega_gjr_t:.6f}\")\n",
    "print(f\"α (alpha):                    {alpha_gjr_t:.6f}\")\n",
    "print(f\"γ (gamma):                    {gamma_gjr_t:.6f}\")\n",
    "print(f\"β (beta):                     {beta_gjr_t:.6f}\")\n",
    "print(f\"\\nTotal impact (negative):      {alpha_gjr_t + gamma_gjr_t:.6f}\")\n",
    "print(f\"Asymmetry ratio:              {(alpha_gjr_t + gamma_gjr_t) / alpha_gjr_t:.3f}\")\n",
    "\n",
    "if gamma_gjr_t > 0:\n",
    "    print(f\"\\n✓ Leverage effect confirmed: γ = {gamma_gjr_t:.6f} > 0\")\n",
    "    print(f\"  Negative shocks increase volatility {((alpha_gjr_t + gamma_gjr_t) / alpha_gjr_t - 1) * 100:.1f}% more\")\n",
    "\n",
    "# Extract conditional volatility\n",
    "gjr_t_vol = gjr_t_fit.conditional_volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'Model':<20} {'Log-Likelihood':>15} {'AIC':>12} {'BIC':>12}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'EWMA':<20} {ewma_fit.loglikelihood:>15.2f} {ewma_fit.aic:>12.2f} {ewma_fit.bic:>12.2f}\")\n",
    "print(f\"{'GARCH(1,1)':<20} {garch_fit.loglikelihood:>15.2f} {garch_fit.aic:>12.2f} {garch_fit.bic:>12.2f}\")\n",
    "print(f\"{'GJR-GARCH(1,1)':<20} {gjr_fit.loglikelihood:>15.2f} {gjr_fit.aic:>12.2f} {gjr_fit.bic:>12.2f}\")\n",
    "print(f\"{'GARCH(1,1)-t':<20} {garch_t_fit.loglikelihood:>15.2f} {garch_t_fit.aic:>12.2f} {garch_t_fit.bic:>12.2f}\")\n",
    "print(f\"{'GJR-GARCH(1,1)-t':<20} {gjr_t_fit.loglikelihood:>15.2f} {gjr_t_fit.aic:>12.2f} {gjr_t_fit.bic:>12.2f}\")\n",
    "print(\"-\"*65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Realized Volatility (RV)\n",
    "\n",
    "### Oxford-Man Realized Volatility Dataset\n",
    "\n",
    "Realized volatility is computed from high-frequency (intraday) data:\n",
    "\n",
    "$$RV_t = \\sum_{i=1}^{N} r_{t,i}^2$$\n",
    "\n",
    "where $r_{t,i}$ are intraday returns.\n",
    "\n",
    "**RV5**: Realized variance based on 5-minute returns\n",
    "\n",
    "**Advantages over GARCH:**\n",
    "- Uses high-frequency information\n",
    "- Model-free measure of volatility\n",
    "- More accurate proxy for true (latent) volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv5_vol = pd.read_csv('sp500rv5.csv', index_col=0, parse_dates=True)\n",
    "rv5_vol.index = pd.to_datetime(rv5_vol.index, utc=True).tz_localize(None).normalize()\n",
    "\n",
    "rv5_vol = rv5_vol['rv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align RV with returns data\n",
    "rv5_aligned = rv5_vol.reindex(returns.index).dropna()\n",
    "\n",
    "print(f\"Aligned observations: {len(rv5_aligned)}\")\n",
    "print(f\"Date range: {rv5_aligned.index[0]} to {rv5_aligned.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RV5 vs GARCH volatility\n",
    "# Align GARCH volatility with RV5 dates\n",
    "garch_vol_aligned = garch_vol.reindex(rv5_aligned.index).dropna()\n",
    "rv5_common = rv5_aligned.reindex(garch_vol_aligned.index)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Time series comparison\n",
    "axes[0].plot(rv5_common.index, rv5_common.values, linewidth=0.8, \n",
    "             label='RV5 (Realized Volatility)', alpha=0.7, color='gray')\n",
    "axes[0].plot(garch_vol_aligned.index, garch_vol_aligned.values, linewidth=1.5, \n",
    "             label='GARCH(1,1)', alpha=0.8, color='red')\n",
    "axes[0].set_title('Realized Volatility (RV5) vs GARCH', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Volatility (%)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(garch_vol_aligned.values, rv5_common.values, alpha=0.3, s=10)\n",
    "axes[1].plot([0, garch_vol_aligned.max()], [0, garch_vol_aligned.max()], \n",
    "             'r--', linewidth=2, label='Perfect Forecast')\n",
    "correlation = np.corrcoef(garch_vol_aligned.values, rv5_common.values)[0, 1]\n",
    "axes[1].set_title(f'GARCH vs RV5 (Correlation = {correlation:.3f})', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('GARCH Volatility (%)')\n",
    "axes[1].set_ylabel('Realized Volatility (%)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Forecast errors\n",
    "errors = rv5_common.values - garch_vol_aligned.values\n",
    "axes[2].plot(rv5_common.index, errors, linewidth=0.8, color='purple')\n",
    "axes[2].axhline(y=0, color='red', linestyle='--', linewidth=0.5)\n",
    "axes[2].set_title('GARCH Forecast Errors (RV5 - GARCH)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Error (%)')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error statistics\n",
    "mse = mean_squared_error(rv5_common, garch_vol_aligned)\n",
    "mae = mean_absolute_error(rv5_common, garch_vol_aligned)\n",
    "print(f\"\\nGARCH Forecast Evaluation:\")\n",
    "print(f\"MSE:  {mse:.6f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse):.6f}\")\n",
    "print(f\"MAE:  {mae:.6f}\")\n",
    "print(f\"Correlation: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HAR Model (Heterogeneous Autoregressive Model)\n",
    "\n",
    "### Reference: Corsi (2009)\n",
    "\n",
    "The HAR model (Corsi, 2009) is a simple but effective approach for forecasting realized volatility.\n",
    "\n",
    "**Model Specification:**\n",
    "\n",
    "$$RV_{t+1} = \\beta_0 + \\beta_D RV_t + \\beta_W RV_t^{(w)} + \\beta_M RV_t^{(m)} + \\varepsilon_{t+1}$$\n",
    "\n",
    "where:\n",
    "- $RV_t$: Daily realized volatility\n",
    "- $RV_t^{(w)} = \\frac{1}{5}\\sum_{i=0}^{4} RV_{t-i}$: Weekly average (5 days)\n",
    "- $RV_t^{(m)} = \\frac{1}{22}\\sum_{i=0}^{21} RV_{t-i}$: Monthly average (22 days)\n",
    "\n",
    "**Key Idea:**\n",
    "- Different market participants have different trading horizons\n",
    "- Daily traders, weekly traders, monthly traders\n",
    "- This heterogeneity creates long memory in volatility\n",
    "\n",
    "**Advantages:**\n",
    "- Simple OLS estimation\n",
    "- No distributional assumptions\n",
    "- Often outperforms complex models\n",
    "- Captures long memory properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare HAR model features\n",
    "print(\"Preparing HAR model features...\\n\")\n",
    "\n",
    "# Use the full SPX RV5 data\n",
    "rv_df = pd.DataFrame({'rv': rv5_vol})\n",
    "\n",
    "# Calculate weekly and monthly averages\n",
    "rv_df['rv_daily'] = rv_df['rv']\n",
    "rv_df['rv_weekly'] = rv_df['rv'].rolling(window=5, min_periods=5).mean()\n",
    "rv_df['rv_monthly'] = rv_df['rv'].rolling(window=22, min_periods=22).mean()\n",
    "\n",
    "# Shift features by 1 for forecasting (use t-1 to predict t)\n",
    "rv_df['rv_daily_lag'] = rv_df['rv_daily'].shift(1)\n",
    "rv_df['rv_weekly_lag'] = rv_df['rv_weekly'].shift(1)\n",
    "rv_df['rv_monthly_lag'] = rv_df['rv_monthly'].shift(1)\n",
    "\n",
    "# Target variable (RV at time t)\n",
    "rv_df['rv_target'] = rv_df['rv']\n",
    "\n",
    "# Drop NaN values\n",
    "rv_df_clean = rv_df.dropna()\n",
    "\n",
    "print(f\"HAR model data prepared:\")\n",
    "print(f\"Total observations: {len(rv_df_clean)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(rv_df_clean[['rv_target', 'rv_daily_lag', 'rv_weekly_lag', 'rv_monthly_lag']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = rv_df_clean.copy()\n",
    "\n",
    "print(f\"Training set: {len(train_df)} observations ({train_df.index[0]} to {train_df.index[-1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate HAR model using OLS\n",
    "print(\"\\nEstimating HAR model using OLS...\\n\")\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = train_df[['rv_daily_lag', 'rv_weekly_lag', 'rv_monthly_lag']]\n",
    "y_train = train_df['rv_target']\n",
    "\n",
    "\n",
    "# Fit HAR model\n",
    "har_model = LinearRegression()\n",
    "har_model.fit(X_train, y_train)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"=\"*60)\n",
    "print(\"HAR MODEL ESTIMATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: RV(t) = β₀ + βD·RV(t-1) + βW·RV_weekly(t-1) + βM·RV_monthly(t-1)\\n\")\n",
    "print(f\"Intercept (β₀):  {har_model.intercept_:>10.6f}\")\n",
    "print(f\"Daily (βD):      {har_model.coef_[0]:>10.6f}\")\n",
    "print(f\"Weekly (βW):     {har_model.coef_[1]:>10.6f}\")\n",
    "print(f\"Monthly (βM):    {har_model.coef_[2]:>10.6f}\")\n",
    "print(f\"\\nSum of coefficients: {har_model.coef_.sum():.6f}\")\n",
    "\n",
    "# In-sample fit\n",
    "y_train_pred = har_model.predict(X_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nIn-sample performance:\")\n",
    "print(f\"  R²:   {r2_train:.6f}\")\n",
    "print(f\"  MSE:  {mse_train:.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mse_train):.6f}\")\n",
    "print(f\"  MAE:  {mae_train:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HAR model forecasts\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# In-sample fit\n",
    "ax.plot(train_df.index, y_train.values, linewidth=0.8, \n",
    "             label='Realized RV5', alpha=0.6, color='black')\n",
    "ax.plot(train_df.index, y_train_pred, linewidth=1, \n",
    "             label='HAR Forecast', alpha=0.8, color='blue')\n",
    "ax.set_title(f'In-Sample Fit (R² = {r2_train:.4f})', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Volatility (%)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. HAR-X Model with VIX\n",
    "\n",
    "### HAR with Exogenous Variables\n",
    "\n",
    "The HAR-X model extends the basic HAR by including exogenous variables that may contain additional information about future volatility.\n",
    "\n",
    "**Model Specification:**\n",
    "\n",
    "$$RV_{t+1} = \\beta_0 + \\beta_D RV_t + \\beta_W RV_t^{(w)} + \\beta_M RV_t^{(m)} + \\beta_{VIX} VIX_t + \\varepsilon_{t+1}$$\n",
    "\n",
    "**VIX (CBOE Volatility Index):**\n",
    "- Implied volatility from S&P 500 options\n",
    "- Forward-looking measure of market expectations\n",
    "- Often called the \"fear index\"\n",
    "- May contain information not captured by historical RV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to WRDS and download CBOE VIX\n",
    "import wrds\n",
    "\n",
    "# Connect to WRDS (you might have to confirm 2FA)\n",
    "db = wrds.Connection(wrds_username=None)\n",
    "\n",
    "# Download CBOE VIX index data from CBOE\n",
    "# Date range: 2000-01-03 to 2022-06-28 (matching RV5 data)\n",
    "query = \"\"\"\n",
    "SELECT date, vix\n",
    "FROM cboe.cboe\n",
    "WHERE date >= '2000-01-03' \n",
    "  AND date <= '2022-06-28'\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "\n",
    "print(\"Downloading CBOE VIX data from WRDS/CBOE...\")\n",
    "vix = db.raw_sql(query)\n",
    "db.close()\n",
    "\n",
    "# Process data\n",
    "vix['date'] = pd.to_datetime(vix['date'])\n",
    "vix = vix.set_index('date')\n",
    "vix = vix[~vix.index.duplicated(keep='first')]\n",
    "vix = vix['vix'].dropna()\n",
    "\n",
    "print(f\"\\nData loaded: {len(vix)} observations\")\n",
    "print(f\"Date range: {vix.index[0]} to {vix.index[-1]}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(vix.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load VIX data from CBOE (alternative)\n",
    "# print(\"Loading VIX data from CBOE...\\n\")\n",
    "\n",
    "# # Fetch VIX historical data\n",
    "# vix_url = 'https://cdn.cboe.com/api/global/us_indices/daily_prices/VIX_History.csv'\n",
    "# vix_data = pd.read_csv(vix_url)\n",
    "\n",
    "# # Process VIX data\n",
    "# vix_data.columns = [col.strip().upper() for col in vix_data.columns]\n",
    "# vix_data['DATE'] = pd.to_datetime(vix_data['DATE'])\n",
    "# vix_data = vix_data.set_index('DATE')\n",
    "\n",
    "# # Select closing VIX\n",
    "# vix = vix_data['CLOSE'].copy()\n",
    "# vix.name = 'vix'\n",
    "\n",
    "# print(f\"VIX data loaded successfully!\")\n",
    "# print(f\"Total observations: {len(vix)}\")\n",
    "# print(f\"Date range: {vix.index.min()} to {vix.index.max()}\")\n",
    "# print(f\"\\nFirst few rows:\")\n",
    "# print(vix.head())\n",
    "# print(f\"\\nVIX statistics:\")\n",
    "# print(vix.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare HAR-X model with VIX\n",
    "print(\"\\nPreparing HAR-X model features...\\n\")\n",
    "\n",
    "# Merge VIX with RV data\n",
    "rv_df_harx = rv_df_clean.copy()\n",
    "rv_df_harx['vix'] = vix.reindex(rv_df_harx.index)\n",
    "\n",
    "# Shift VIX by 1 (use VIX at t-1 to predict RV at t)\n",
    "rv_df_harx['vix_lag'] = rv_df_harx['vix'].shift(1)\n",
    "\n",
    "# Drop NaN values\n",
    "rv_df_harx = rv_df_harx.dropna()\n",
    "\n",
    "print(f\"HAR-X model data prepared:\")\n",
    "print(f\"Total observations: {len(rv_df_harx)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(rv_df_harx[['rv_target', 'rv_daily_lag', 'rv_weekly_lag', 'rv_monthly_lag', 'vix_lag']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_harx = rv_df_harx.copy()\n",
    "\n",
    "print(f\"Training set: {len(train_df_harx)} observations ({train_df_harx.index[0]} to {train_df_harx.index[-1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate HAR-X model using OLS\n",
    "print(\"\\nEstimating HAR-X model using OLS...\\n\")\n",
    "\n",
    "# Prepare features and target\n",
    "X_train_harx = train_df_harx[['rv_daily_lag', 'rv_weekly_lag', 'rv_monthly_lag', 'vix_lag']]\n",
    "y_train_harx = train_df_harx['rv_target']\n",
    "\n",
    "\n",
    "# Fit HAR-X model\n",
    "harx_model = LinearRegression()\n",
    "harx_model.fit(X_train_harx, y_train_harx)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"=\"*60)\n",
    "print(\"HAR-X MODEL ESTIMATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: RV(t) = β₀ + βD·RV(t-1) + βW·RV_weekly(t-1) + βM·RV_monthly(t-1) + βVIX·VIX(t-1)\\n\")\n",
    "print(f\"Intercept (β₀):  {harx_model.intercept_:>10.6f}\")\n",
    "print(f\"Daily (βD):      {harx_model.coef_[0]:>10.6f}\")\n",
    "print(f\"Weekly (βW):     {harx_model.coef_[1]:>10.6f}\")\n",
    "print(f\"Monthly (βM):    {harx_model.coef_[2]:>10.6f}\")\n",
    "print(f\"VIX (βVIX):      {harx_model.coef_[3]:>10.6f}\")\n",
    "\n",
    "# In-sample fit\n",
    "y_train_pred_harx = harx_model.predict(X_train_harx)\n",
    "r2_train_harx = r2_score(y_train_harx, y_train_pred_harx)\n",
    "mse_train_harx = mean_squared_error(y_train_harx, y_train_pred_harx)\n",
    "mae_train_harx = mean_absolute_error(y_train_harx, y_train_pred_harx)\n",
    "\n",
    "print(f\"\\nIn-sample performance:\")\n",
    "print(f\"  R²:   {r2_train_harx:.6f}\")\n",
    "print(f\"  MSE:  {mse_train_harx:.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mse_train_harx):.6f}\")\n",
    "print(f\"  MAE:  {mae_train_harx:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HAR-VIX model \n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# In-sample fit\n",
    "ax.plot(train_df_harx.index, y_train_harx.values, linewidth=0.8, \n",
    "             label='Realized RV5', alpha=0.6, color='black')\n",
    "ax.plot(train_df_harx.index, y_train_pred_harx, linewidth=1, \n",
    "             label='HAR-VIX Forecast', alpha=0.8, color='blue')\n",
    "ax.set_title(f'In-Sample Fit (R² = {r2_train_harx:.4f})', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Volatility (%)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
